{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8.4 循环神经网络\n",
    "这一章节引入了“隐状态”的概念\n",
    "\n",
    "在n元语法模型中，单词$x_t$在时间步$t$的条件概率取决于前面n-1个单词，如果想将时间步$t-(n-1)$之前的单词的影响合并到$x_t$上，需要增大n，这也会使模型的参数呈指数级增长。因此引入隐变量模型：\n",
    "$$P(x_t|x_{t-1},...,x_1)≈P(x_t|h_{t-1})$$\n",
    "其中，$h_{t-1}$是隐状态，也称为隐藏变量，其存储了到时间步$t-1$的序列信息。\n",
    "\n",
    "可以基于当前的输入$x_t$和之前的隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：\n",
    "$$h_t=f(x_t,h_{t-1})$$"
   ],
   "id": "aa7ac5ff92144e98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8.4.2 有隐状态的循环神经网络\n",
    "简而言之，输出 = 激活(输入x权重 + 隐状态x权重 + 偏置)\n",
    "\n",
    "这也解释了循环神经网络名称的来源：在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是循环的。基于此，该神经网络被称为循环神经网络"
   ],
   "id": "19f07bada459ba58"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T10:24:01.392579Z",
     "start_time": "2025-03-07T10:24:01.375832Z"
    }
   },
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "X,W_xh = torch.normal(0,1,(3,1)),torch.normal(0,1,(1,4))\n",
    "H,W_hh = torch.normal(0,1,(3,4)),torch.normal(0,1,(4,4))\n",
    "torch.matmul(X,W_xh) + torch.matmul(H,W_hh)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3341,  5.4078,  0.7332, -1.5146],\n",
       "        [ 0.3257, -1.9553,  0.3642,  2.3388],\n",
       "        [ 2.9765, -1.7088, -0.2650, -2.2089]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T10:25:29.483912Z",
     "start_time": "2025-03-07T10:25:29.475283Z"
    }
   },
   "cell_type": "code",
   "source": "torch.matmul(torch.cat((X,H),1),torch.cat((W_xh,W_hh),0))",
   "id": "a28c8ad0e8dff5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3341,  5.4078,  0.7332, -1.5146],\n",
       "        [ 0.3257, -1.9553,  0.3642,  2.3388],\n",
       "        [ 2.9765, -1.7088, -0.2650, -2.2089]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8.4.3 基于RNN的字符集语言模型\n",
    "在8.3中提到了，理解就行"
   ],
   "id": "da8bf0e914a4e5fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8.4.4 困惑度\n",
    "一个序列中所有的n个词元的交叉熵损失的平均值：\n",
    "$$\\frac{1}{n}\\sum\\limits_{t = 1}^n { - \\log P\\left( {{x_t}|{x_{t - 1}},...,{x_1}} \\right)} $$\n",
    "困惑度基于上式定义：\n",
    "$$\\exp \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n { - \\log P\\left( {{x_t}|{x_{t - 1}},...,{x_1}} \\right)} } \\right)$$\n",
    "对“困惑度”的理解可以为“下一个词元的实际选择数的调和平均数”，比如：\n",
    "- 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1\n",
    "- 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，模型的困惑度为正无穷大\n",
    "- 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量"
   ],
   "id": "3d7a94ada83172f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
